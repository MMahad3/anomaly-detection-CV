{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XWUPRH3YL9v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "import seaborn as sns\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "pyf01fdJVpcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 12\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1\n",
        "LR =  0.0001\n",
        "NUM_CLASSES = 14\n",
        "train_dir = \"./drive/MyDrive/train_dataset2.npz\"\n",
        "test_dir = \"./drive/MyDrive/test_dataset2.npz\"\n",
        "IMG_HEIGHT = 64\n",
        "IMG_WIDTH = 64"
      ],
      "metadata": {
        "id": "f1lvgLj6Kam3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_train = np.load(train_dir)\n",
        "data_test = np.load(test_dir)\n",
        "X_train = data_train['X']\n",
        "X_test = data_test['X']\n",
        "y_train = data_train['y']\n",
        "y_test = data_test['y']\n",
        "CLASS_LABELS = ['Abuse','Arrest','Arson','Assault','Burglary','Explosion','Fighting',\"Normal\",'RoadAccidents','Robbery','Shooting','Shoplifting','Stealing','Vandalism']\n",
        "\n",
        "print(f\"Loaded dataset:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dnCzckDMXMLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6246a1-1348-449c-fb78-4c42f97fc98c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset:\n",
            "X_train shape: (354626, 64, 64, 3)\n",
            "X_test shape: (41397, 64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Find class distributions\n",
        "class_counts = Counter(y_train)\n",
        "print(\"Class Distribution:\", class_counts)\n",
        "\n",
        "# Separate samples by class\n",
        "class_samples = {cls: X_train[y_train == cls] for cls in np.unique(y_train)}\n"
      ],
      "metadata": {
        "id": "DsHVFlY2IyDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c77da9-b9a6-4532-f4b6-ea436b5435bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution: Counter({7: 150000, 12: 30000, 9: 28000, 4: 18000, 6: 18000, 11: 18000, 8: 15000, 13: 13626, 1: 13000, 2: 13000, 5: 12000, 3: 10000, 0: 9000, 10: 7000})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_fun = tf.keras.applications.densenet.preprocess_input\n",
        "# Define data augmentation for each class type\n",
        "minority_augmenter = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=(0.8, 1.2)\n",
        ")\n",
        "\n",
        "moderate_augmenter = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# No augmentation for the majority class\n",
        "no_augmenter = ImageDataGenerator()\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                  preprocessing_function=preprocess_fun\n",
        "                                 )"
      ],
      "metadata": {
        "id": "eXChOH4IKwxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to augment a class\n",
        "def augment_class(augmenter, class_samples, target_size):\n",
        "    augmented_samples = []\n",
        "    for img in class_samples:\n",
        "        # Reshape image for augmentation (add batch dimension)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        # Generate augmented images\n",
        "        for _ in range(target_size - len(class_samples)):  # Generate only the required number of samples\n",
        "            augmented_img = next(augmenter.flow(img, batch_size=1))[0]\n",
        "            augmented_samples.append(augmented_img)\n",
        "    return np.array(augmented_samples)\n",
        "\n",
        "# Augment minority and moderate classes\n",
        "balanced_data = []\n",
        "balanced_labels = []\n",
        "\n",
        "for cls, samples in class_samples.items():\n",
        "    if class_counts[cls] < 50:  # Example: Minority class\n",
        "        augmented = augment_class(minority_augmenter, samples, target_size=100)  # Target 100 samples\n",
        "        balanced_data.append(np.concatenate([samples, augmented]))\n",
        "    elif 50 <= class_counts[cls] < 200:  # Moderate class\n",
        "        augmented = augment_class(moderate_augmenter, samples, target_size=200)  # Target 200 samples\n",
        "        balanced_data.append(np.concatenate([samples, augmented]))\n",
        "    else:  # Majority class, no augmentation\n",
        "        balanced_data.append(samples)\n",
        "\n",
        "    # Create labels for augmented data\n",
        "    balanced_labels.append(np.full((len(balanced_data[-1]),), cls))\n"
      ],
      "metadata": {
        "id": "n2mDPs2JKVJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all augmented data and labels\n",
        "X_train_balanced = np.concatenate(balanced_data, axis=0)\n",
        "y_train_balanced = np.concatenate(balanced_labels, axis=0)\n",
        "\n",
        "print(\"New Balanced Class Distribution:\", Counter(y_train_balanced))\n"
      ],
      "metadata": {
        "id": "ZWqrTYUqLsOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "X_train_balanced, y_train_balanced = shuffle(X_train_balanced, y_train_balanced, random_state=42)\n"
      ],
      "metadata": {
        "id": "pH41bUbML1Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    horizontal_flip=True,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow(\n",
        "    X_train_balanced,\n",
        "    y_train_balanced,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(directory = test_dir,\n",
        "                                                   target_size = (IMG_HEIGHT ,IMG_WIDTH),\n",
        "                                                    batch_size = BATCH_SIZE,\n",
        "                                                    shuffle  = False ,\n",
        "                                                    color_mode = \"rgb\",\n",
        "                                                    class_mode = \"categorical\",\n",
        "                                                    seed = SEED\n",
        "                                                  )"
      ],
      "metadata": {
        "id": "y13gYqTaMBKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(x = CLASS_LABELS,\n",
        "             y = [list(train_generator.classes).count(i) for i in np.unique(train_generator.classes)] ,\n",
        "             color = np.unique(train_generator.classes) ,\n",
        "             color_continuous_scale=\"Emrld\")\n",
        "fig.update_xaxes(title=\"Classes\")\n",
        "fig.update_yaxes(title = \"Number of Images\")\n",
        "fig.update_layout(showlegend = True,\n",
        "    title = {\n",
        "        'text': 'Train Data Distribution ',\n",
        "        'y':0.95,\n",
        "        'x':0.5,\n",
        "        'xanchor': 'center',\n",
        "        'yanchor': 'top'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "v0-hZ16_lusV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(x = CLASS_LABELS,\n",
        "             y = [list(test_generator.classes).count(i) for i in np.unique(test_generator.classes)] ,\n",
        "             color = np.unique(train_generator.classes) ,\n",
        "             color_continuous_scale=\"Emrld\")\n",
        "fig.update_xaxes(title=\"Classes\")\n",
        "fig.update_yaxes(title = \"Number of Images\")\n",
        "fig.update_layout(showlegend = True,\n",
        "    title = {\n",
        "        'text': 'Test Data Distribution ',\n",
        "        'y':0.95,\n",
        "        'x':0.5,\n",
        "        'xanchor': 'center',\n",
        "        'yanchor': 'top'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "c2AOUhAklzlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sample(X,y,index):\n",
        "    plt.figure(figsize=(3,2))\n",
        "    plt.imshow(X[index])\n",
        "    plt.xlabel(CLASS_LABELS[index])\n",
        "\n",
        "plot_sample(X_train,y_train,1)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H_BrrnNYXRpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGAcHt42VmrC"
      },
      "outputs": [],
      "source": [
        "def create_hybrid_vit_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):\n",
        "    # 1. CNN Feature Extractor\n",
        "    cnn_input = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(cnn_input)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    cnn_features = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "    # 2. Transformer Input: Patch Embeddings\n",
        "    image_size = input_shape[0]\n",
        "    patch_size = 6\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    projection_dim = 64\n",
        "\n",
        "    # Adjust the CNN output to match num_patches * projection_dim\n",
        "    adjusted_dim = num_patches * projection_dim\n",
        "    cnn_features = layers.Dense(adjusted_dim, activation='relu')(cnn_features)\n",
        "\n",
        "    # Reshape into patches\n",
        "    patches = layers.Reshape((num_patches, projection_dim))(cnn_features)\n",
        "\n",
        "    # 3. Positional Encoding\n",
        "    position_embedding = tf.constant(tf.random.uniform((1, num_patches, projection_dim)))\n",
        "    embedded_patches = patches + position_embedding\n",
        "\n",
        "    # 4. Transformer Layers\n",
        "    for _ in range(8):  # 8 transformer layers\n",
        "        attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=projection_dim)(\n",
        "            embedded_patches, embedded_patches\n",
        "        )\n",
        "        attention_output = layers.Add()([attention_output, embedded_patches])  # Residual connection\n",
        "        attention_output = layers.LayerNormalization()(attention_output)\n",
        "\n",
        "        mlp_output = layers.Dense(projection_dim * 2, activation='relu')(attention_output)\n",
        "        mlp_output = layers.Dense(projection_dim)(mlp_output)\n",
        "        embedded_patches = layers.Add()([mlp_output, attention_output])  # Residual connection\n",
        "        embedded_patches = layers.LayerNormalization()(embedded_patches)\n",
        "\n",
        "    # 5. Classification Head\n",
        "    representation = layers.GlobalAveragePooling1D()(embedded_patches)\n",
        "    mlp_head = layers.Dense(2048, activation='relu')(representation)\n",
        "    mlp_head = layers.Dense(1024, activation='relu')(mlp_head)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(mlp_head)\n",
        "\n",
        "    hybrid_vit_model = Model(inputs=cnn_input, outputs=output, name=\"Hybrid_CNN_ViT\")\n",
        "    return hybrid_vit_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate and compile the model\n",
        "hybrid_vit = create_hybrid_vit_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
        "hybrid_vit.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Summarize the model\n",
        "hybrid_vit.summary()\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_balanced),\n",
        "    y=y_train_balanced\n",
        ")\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#     rotation_range=15,\n",
        "#     width_shift_range=0.1,\n",
        "#     height_shift_range=0.1,\n",
        "#     horizontal_flip=True\n",
        "# )\n",
        "# datagen.fit(X_train)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = hybrid_vit.fit(\n",
        "    x=train_generator,\n",
        "    validation_data=test_generator,\n",
        "    epochs=EPOCHS,\n",
        "    # callbacks=[early_stopping],\n",
        "    class_weight=class_weights_dict\n",
        ")\n",
        "\n"
      ],

      "metadata": {
        "id": "G_TvZX0IYRxY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "LyWkXgpKYXOJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions and convert them to class indices\n",
        "y_pred = hybrid_vit.predict(X_test)\n",
        "y_pred_classes = [np.argmax(element) for element in y_pred]\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"classification_report\\n\", classification_report(y_test, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "LjfX8Eoesy5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "hybrid_vit.save('./drive/MyDrive/FYP/hybrid_vit_model.h5')\n"
      ],
      "metadata": {
        "id": "RUZwxMMsnva5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sample_with_prediction(X, y_true, y_pred, index):\n",
        "    plt.figure(figsize=(3, 2))\n",
        "    plt.imshow(X[index])\n",
        "    plt.title(f\"True: {class_names[y_true[index]]}, Pred: {class_names[y_pred[index]]}\")\n",
        "    plt.show()\n",
        "\n",
        "plot_sample_with_prediction(X_test, y_test, y_pred_classes, 661)\n"
      ],
      "metadata": {
        "id": "yOpEaI4GrBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJnHbo3F-kCk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
